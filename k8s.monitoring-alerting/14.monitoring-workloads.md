
## Monitoring Kubernetes Workloads

```
- Monitoring Kubernetes With Golden Signals

    - Golden Signals are a standard for Kubernetes application monitoring. 
    - These signals help troubleshoot microservices applications with a
      reduced set of metrics that offer a wide view of a service 
      from a user or consumer perspective


- Monitoring Kubernetes Cluster and nodes use cases
        - Host is down. 
            - If a host is down or unreachable, you might want to receive a notification
              with a suitable wait time to avoid noisy alerts.

        - Do I have enough Kubernetes nodes in the cluster?
            - A node failure isn’t a problem in Kubernetes since the scheduler will spawn
             the containers from the pods in the failed node into other available nodes. 

        - But what if you  are running out of nodes, or the resource requirements
          for the deployed applications overbook existing nodes?
          and what if you are hitting a quota limit?



- Monitoring Kubernetes Control Plane
        - The Kubernetes control plane is the brain of your Kubernetes cluster. 
          It manages all of your cluster resources, can schedule new pods, and 
          can read all of the secrets stored in the cluster. 
        - The main components of the Kubernetes control plane are:
            - API Server 
            - etcd
            - Controller manager • Kube-dns
            - Kubelet 
            - Kube-proxy

- Benefits of monitoring Kubernetes control plane:
        - Node The control plane controls your cluster; monitoring it,
          especially the kube-apiserver, will let you 
                - detect a latency, errors, and validate the service performance. 
        - Monitoring the Kubernetes API server provides visibility into 
          all of the communication between the cluster components.

        - Monitoring API server use cases:
                - You detect latency increase in the requests to the API. 
                    - This is typically a sign of overload in the API server. 
                    - Your cluster is most likely loaded and the API server may need to be scaled out.
                - You detect an increase in the depth and latency of the work queue.
                    - You are having issues scheduling actions. 
                    - You should check that the scheduler is working. 
                    - Perhaps one node is having issues and you may want to replace it.

        - Kube-controller-manager is responsible for having the correct number of elements in all
            of the deployments, daemonsets, persistent volume claims, and many other Kubernetes elements. 
            - An issue in the kube-controller manager can compromise scalability and
            resilience of the applications running in the cluster. 
            - Monitoring the kube-controller manager can prevent complications that 
            would be hard to detect otherwise


- Monitoring Kubelet use cases:
       
       - Pods are not starting. 
            - This is typically a sign of Kubelet having problems connecting to the container runtime
              running below. 
            - Check the pod start rate and duration metrics to see if there is latency 
              creating the containers or if they are, in fact, starting.
        
        - A node doesn’t seem to be scheduling new pods.
          - Check the Kubelet job number. 
            There’s a chance that kubelet has died in a node and is unable to schedule pods.
        
        - Kubernetes seems to be slow performing operations.
          - Check all of the golden signals in kubelet metrics. 
          - It may have issues with 
                    - storage 
                    - latency 
                    - load issues
                    - communicating  with the container runtime engine

- Applications in Kubernetes are structured into several hierarchical layers 
  that define how the services are organized outside of the Kubernetes world.

    - Physical 
        - node 
            - Master (Control Plane)
            - Worker (Data plane)
    - Logical
            - Namespace/Workloads
                - deployment
                    - replicaset
                        - pod <- service
                - daemonset
                - statefulset
                - jobs/cronjobs
            

- Monitoring Kubernetes application use cases:
        - Do I have enough pods/containers running for each  application? 
                - There are multiple reasons why the number of running containers can change. 
                - That includes rescheduling containers in a different host because a
                  node failed, or because there aren’t enough resources and the pod was evicted 
                  from a rolling deployment of a new version, and more.

        - Do I have any pod/containers for a given application?
                - Get an alert if there aren’t any containers running for a given application.

        - Is there any pod/container in a restart loop? When
                - deploying a new version that’s broken, if there aren’t enough resources available or 
                  some requirements/dependencies aren’t in place, you might end up with
                  a container or pod continuously restarting in a loop.
                        That’s called CrashLoopBackOff. When this happens,
                        pods never get into ready status and, therefore, are
                        counted as unavailable and not as running.

- Namespaces Quotas
    - Set requests and limits in your workloads.
    - Setting a namespace quota will enforce all of the workloads in 
      the namespace to have a request and   limit in every container.

- Kubernetes OOM problems

        - When a node is low on memory, Kubernetes evic-tion policy steps in and fails/stops pods. 

        - If they are managed by a ReplicaSet, these pods are scheduled in a different node. 
        
        It’s important to monitor closely to resolve issues like:
                - OOM kill due to container limit reached
                - Kubernetes OOM kill due to limit overcommit
                - CPU throttling due to CPU limit

1. Monitor resource usage in your workloads -   
    - This will allow you to discover  different issues that can affect 
      the health of the applications running in the cluster.

2. Understanding that your resource usage can compromise your application and affect other
   applications in the cluster is the crucial first step. 
   You have to properly configure your quotas.

3. Monitoring the resources and how they are related to the limits and requests will 
   help you set reasonable values and avoid Kubernetes OOM kills. 
   
   This will result in a better performance of all the applications in the cluster, 
   as well as a fair sharing of resources.

4. Your Kubernetes alerting strategy can’t just focus on the infrastructure layer. 
   It needs to understand the entire stack, from the hosts and 
   Kubernetes nodes at the bottom, up to the top where the application workloads run.

5. Being able to leverage Kubernetes and cloud providers metadata to aggregate and segment
   metrics and alerts will be a requirement for effective alerting across all layers.

```